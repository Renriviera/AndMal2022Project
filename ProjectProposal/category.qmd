---
title: "Category Prediction Model"
format: html
---

This document trains and evaluates a random forest model to predict the malware category using features from the AndMal2020 dataset.

## Load Libraries and Data

```{r}
#| label: load-libraries
#| cache: true

library(ranger)
library(dplyr)
library(caret)
library(parallel)
library(lime)
library(fastshap)
library(ggplot2)
library(stringr)
library(kableExtra)

# Configuration
num_threads <- 8L
num_trees <- 500L
train_test_split <- 0.8
```

```{r}
#| label: load-processed-data
#| cache: true

# Load preprocessed data (from preprocessing.qmd)
andmal_after <- readRDS("data/processed/andmal_after.rds")

cat(sprintf("Loaded dataset: %d rows, %d columns\n", nrow(andmal_after), ncol(andmal_after)))
cat("Category distribution:\n")
print(table(andmal_after$Category))
```

## Feature Selection

```{r}
#| label: identify-features
#| cache: true

# Metadata columns to exclude from features
metadata_cols <- c("Category", "Family", "Category_file", "reboot_state", "path", "file", "Hash")

# Get all column names
all_cols <- names(andmal_after)

# Identify metadata columns that actually exist
metadata_cols_present <- intersect(metadata_cols, all_cols)

# Feature columns for Category prediction (exclude Family and Category, plus other metadata)
feature_cols_model1 <- setdiff(all_cols, metadata_cols_present)

# Additional exclusion: any rank/color columns that might have been added
feature_cols_model1 <- feature_cols_model1[!str_detect(feature_cols_model1, "rank|color|fam_color|rank_in_cat")]

cat(sprintf("Model features (Category prediction): %d features\n", length(feature_cols_model1)))
cat(sprintf("Excluded metadata columns: %s\n", paste(metadata_cols_present, collapse = ", ")))
```

## Train/Test Split

```{r}
#| label: train-test-split
#| cache: true

# Create stratified split for Category
set.seed(42)
caret_available <- require("caret", quietly = TRUE)

if (caret_available) {
  train_index <- caret::createDataPartition(
    andmal_after$Category,
    p = train_test_split,
    list = FALSE,
    times = 1
  )
  train_data <- andmal_after[train_index, ]
  test_data <- andmal_after[-train_index, ]
} else {
  # Use base R stratified sampling
  categories <- unique(andmal_after$Category)
  train_indices <- c()
  for (cat in categories) {
    cat_indices <- which(andmal_after$Category == cat)
    n_train <- round(length(cat_indices) * train_test_split)
    train_cat_indices <- sample(cat_indices, n_train)
    train_indices <- c(train_indices, train_cat_indices)
  }
  train_data <- andmal_after[train_indices, ]
  test_data <- andmal_after[-train_indices, ]
}

cat(sprintf("Training set: %d rows (%.1f%%)\n", nrow(train_data), 100 * nrow(train_data) / nrow(andmal_after)))
cat(sprintf("Test set: %d rows (%.1f%%)\n", nrow(test_data), 100 * nrow(test_data) / nrow(andmal_after)))
```

## Train Model

```{r}
#| label: train-model
#| cache: false

# Prepare data
train_model1_x <- train_data[, feature_cols_model1, drop = FALSE]
train_model1_y <- train_data$Category
test_model1_x <- test_data[, feature_cols_model1, drop = FALSE]
test_model1_y <- test_data$Category

# Calculate mtry (default: sqrt of number of features)
mtry_model1 <- floor(sqrt(length(feature_cols_model1)))

cat(sprintf("Model parameters:\n"))
cat(sprintf("  Number of trees: %d\n", num_trees))
cat(sprintf("  mtry: %d (sqrt of %d features)\n", mtry_model1, length(feature_cols_model1)))
cat(sprintf("  Number of threads: %d\n", num_threads))

# Train model
cat("Training random forest for Category prediction...\n")
start_time <- Sys.time()

rf_category <- ranger(
  x = train_model1_x,
  y = train_model1_y,
  num.trees = num_trees,
  mtry = mtry_model1,
  min.node.size = 1,
  num.threads = num_threads,
  classification = TRUE,
  probability = TRUE,
  importance = "impurity",
  verbose = TRUE
)

end_time <- Sys.time()
training_time <- difftime(end_time, start_time, units = "mins")
cat(sprintf("Training completed in %.2f minutes\n", as.numeric(training_time)))
```

## Evaluate Model

```{r}
#| label: evaluate-model
#| cache: false

cat("Evaluating model...\n")
pred_model1 <- predict(rf_category, test_model1_x)
pred_categories <- pred_model1$predictions

# Get predicted class (highest probability)
pred_categories_class <- colnames(pred_categories)[apply(pred_categories, 1, which.max)]
pred_categories_class <- factor(pred_categories_class, levels = levels(test_model1_y))

# Confusion matrix (always use caret)
cm_model1 <- caret::confusionMatrix(pred_categories_class, test_model1_y)

# Calculate per-class metrics
metrics_model1 <- cm_model1$overall
per_class_model1 <- cm_model1$byClass

cat("\nOverall Accuracy:", metrics_model1["Accuracy"], "\n")
```

### Model Performance Metrics

```{r}
#| label: category-metrics-table
#| echo: false

# Extract metrics from confusion matrix (caret is required)
# Overall accuracy
overall_acc <- as.numeric(cm_model1$overall["Accuracy"])

# Per-class metrics (precision, recall, F1)
if (!is.null(per_class_model1)) {
  # Extract per-class metrics
  precision <- per_class_model1[, "Precision"]
  recall <- per_class_model1[, "Recall"]
  f1 <- per_class_model1[, "F1"]
  
  # Create metrics table
  metrics_table <- data.frame(
    Category = names(precision),
    Accuracy = rep(overall_acc, length(precision)),
    Precision = as.numeric(precision),
    Recall = as.numeric(recall),
    F1 = as.numeric(f1)
  )
} else {
  stop("Per-class metrics not available. Please ensure caret package is properly loaded.")
}

# Format table with kableExtra
metrics_table %>%
  kbl(
    caption = "Category Prediction Model Performance Metrics",
    col.names = c("Category", "Overall Accuracy", "Precision", "Recall", "F1 Score"),
    digits = 4,
    align = c("l", "c", "c", "c", "c")
  ) %>%
  kable_styling(
    bootstrap_options = c("striped", "hover", "condensed"),
    full_width = FALSE,
    position = "center"
  ) %>%
  row_spec(0, bold = TRUE, background = "#4472C4", color = "white")
```

### Performance Metrics

```{r}
#| label: display-metrics
#| echo: false

cat("Overall Metrics:\n")
if (caret_available) {
  print(metrics_model1)
  if (!is.null(per_class_model1)) {
    cat("\nPer-Class Metrics:\n")
    print(per_class_model1)
  }
} else {
  cat("Accuracy:", metrics_model1$Accuracy, "\n")
}
```

### Feature Importance

```{r}
#| label: feature-importance
#| echo: false

if (!is.null(rf_category$variable.importance)) {
  importance_df <- data.frame(
    feature = names(rf_category$variable.importance),
    importance = as.numeric(rf_category$variable.importance)
  ) %>%
    arrange(desc(importance)) %>%
    head(20)
  
  cat("Top 20 Most Important Features:\n")
  print(importance_df)
}
```

## Save Model

```{r}
#| label: save-model
#| cache: false
#| eval: true

saveRDS(rf_category, "data/models/rf_category_model.rds")
cat("Saved: data/models/rf_category_model.rds\n")
```

## Random Forest Model Summary

The random forest model for category prediction demonstrates strong performance in distinguishing between malware categories. The model utilizes a comprehensive set of behavioral features extracted from dynamic analysis to classify samples into their respective categories. The ensemble approach of random forests allows the model to capture complex interactions between features while maintaining interpretability through feature importance measures.

```{r}
#| label: category-model-summary
#| echo: false

cat("Model Configuration:\n")
cat(sprintf("  Number of trees: %d\n", rf_category$num.trees))
cat(sprintf("  Number of features: %d\n", length(feature_cols_model1)))
cat(sprintf("  mtry parameter: %d\n", rf_category$mtry))
cat(sprintf("  Training accuracy: %.4f\n", 1 - rf_category$prediction.error))
```

## Model Interpretability

### Instance Selection

```{r}
#| label: select-instances
#| cache: true

# Set seed for reproducibility
set.seed(42)

# Select one instance from each category from the test set
target_categories <- c("Adware", "Riskware", "Trojan")
selected_instances <- list()

for (category in target_categories) {
  category_indices <- which(test_data$Category == category)
  if (length(category_indices) > 0) {
    # Select first instance from this category in test set
    selected_idx <- category_indices[1]
    selected_instances[[category]] <- test_data[selected_idx, ]
    cat(sprintf("Selected %s instance: row %d\n", category, selected_idx))
  } else {
    cat(sprintf("WARNING: No %s instances found in test set\n", category))
  }
}

# Combine into a single data frame for easier handling
selected_instances_df <- bind_rows(selected_instances)

# Extract feature matrices for explanations
selected_instances_x <- selected_instances_df[, feature_cols_model1, drop = FALSE]

# Save selected instances for use in family.qmd
if (!dir.exists("data/processed")) {
  dir.create("data/processed", recursive = TRUE)
}
saveRDS(selected_instances_df, "data/processed/selected_instances.rds")
cat("\nSaved selected instances to data/processed/selected_instances.rds\n")
```

### LIME Explanations

LIME (Local Interpretable Model-agnostic Explanations) generates local approximations of the model's behavior. These explanations highlight the most influential features for each prediction by creating simplified, interpretable models that approximate the complex random forest in the vicinity of each instance. This approach allows us to understand the decision-making process at the individual sample level, revealing which behavioral features are most critical for distinguishing between malware categories.

```{r}
#| label: lime-explanations
#| cache: false
#| fig.width: 12
#| fig.height: 8

# Set seed for reproducibility
set.seed(42)

# Create a prediction function wrapper for ranger model
predict_function <- function(model, newdata) {
  pred <- predict(model, newdata)
  return(pred$predictions)
}

# Create LIME explainer
cat("Creating LIME explainer...\n")
explainer <- lime(
  train_model1_x,
  model = rf_category,
  bin_continuous = TRUE,
  n_bins = 5
)

# Generate explanations for all selected instances
cat("Generating LIME explanations for selected instances...\n")
lime_explanations <- lime::explain(
  selected_instances_x,
  explainer = explainer,
  n_features = 10,
  n_permutations = 5000,
  n_labels = 1
)

# Plot LIME explanations
cat("Plotting LIME explanations...\n")
for (i in 1:nrow(selected_instances_x)) {
  category_name <- selected_instances_df$Category[i]
  cat(sprintf("\nLIME Explanation for %s instance:\n", category_name))
  print(plot_features(lime_explanations[lime_explanations$case == i, ]))
}
```

### fastshap Explanations

SHAP (SHapley Additive exPlanations) values provide insight into feature contributions for individual predictions. The analysis reveals which features drive classification decisions for each category instance. By decomposing the model's output into additive feature contributions, SHAP values offer a unified framework for understanding how each feature influences the final prediction, making it possible to explain why a particular sample was classified into a specific category.

#### Per-Instance SHAP Values

```{r}
#| label: shap-per-instance
#| cache: false
#| fig.width: 12
#| fig.height: 8

# Set seed for reproducibility
set.seed(42)

# Create prediction function for fastshap
pred_wrapper <- function(object, newdata) {
  pred <- predict(object, newdata)
  return(pred$predictions)
}

cat("Calculating SHAP values for selected instances...\n")

# Calculate SHAP values for each selected instance
shap_values_list <- list()

for (i in 1:nrow(selected_instances_x)) {
  category_name <- selected_instances_df$Category[i]
  cat(sprintf("Calculating SHAP values for %s instance...\n", category_name))
  
  # Ensure newdata is a data.frame to match training feature class
  newdata_df <- as.data.frame(selected_instances_x[i, , drop = FALSE])
  
  # Calculate SHAP values for this instance
  shap_vals <- explain(
    rf_category,
    X = train_model1_x,
    newdata = newdata_df,
    pred_wrapper = pred_wrapper,
    nsim = 100
  )
  
  shap_values_list[[category_name]] <- shap_vals
  
  # Plot SHAP values for this instance
  # Get the predicted class probabilities
  pred_probs <- predict(rf_category, newdata_df)$predictions
  pred_class <- colnames(pred_probs)[which.max(pred_probs)]
  
  # Handle SHAP values - fastshap returns a matrix/data.frame
  # For multi-class, extract SHAP values for the predicted class
  if (is.data.frame(shap_vals) || is.matrix(shap_vals)) {
    # If it's a matrix/data.frame, check if it has class columns
    if (pred_class %in% colnames(shap_vals)) {
      shap_df <- data.frame(
        feature = rownames(shap_vals),
        shap_value = shap_vals[[pred_class]]
      )
    } else if (ncol(shap_vals) == length(feature_cols_model1)) {
      # If columns are features, use the first (or only) row
      shap_df <- data.frame(
        feature = colnames(shap_vals),
        shap_value = as.numeric(shap_vals[1, ])
      )
    } else {
      # Try to extract first column or first row
      if (nrow(shap_vals) == 1) {
        shap_df <- data.frame(
          feature = colnames(shap_vals),
          shap_value = as.numeric(shap_vals[1, ])
        )
      } else {
        shap_df <- data.frame(
          feature = rownames(shap_vals),
          shap_value = as.numeric(shap_vals[, 1])
        )
      }
    }
    
    shap_df <- shap_df %>%
      arrange(desc(abs(shap_value))) %>%
      head(20)
    
    cat(sprintf("\nTop 20 SHAP values for %s instance (predicted class: %s):\n", 
                category_name, pred_class))
    print(shap_df)
    
    # Determine the order of magnitude (exponent) for scientific notation
    max_abs_value <- max(abs(shap_df$shap_value))
    if (max_abs_value > 0) {
      exponent <- floor(log10(max_abs_value))
      # Round to nearest multiple of 3 for cleaner display
      exponent <- round(exponent / 3) * 3
    } else {
      exponent <- 0
    }
    
    # Create custom label function that shows only significant digits
    # and scales by the exponent
    scale_factor <- 10^(-exponent)
    label_func <- function(x) {
      scaled <- x * scale_factor
      # Format with appropriate decimal places
      if (abs(exponent) >= 3) {
        sprintf("%.3f", scaled)
      } else {
        sprintf("%.4f", scaled)
      }
    }
    
    # Create visualization
    y_axis_label <- if (abs(exponent) >= 3) {
      sprintf("SHAP Value (Ã—10^%d)", exponent)
    } else {
      "SHAP Value"
    }
    
    p_shap <- ggplot(shap_df, aes(x = reorder(feature, shap_value), y = shap_value)) +
      geom_col(aes(fill = shap_value > 0)) +
      scale_fill_manual(
        values = c("TRUE" = "#2E8B57", "FALSE" = "#DC143C"),
        labels = c("TRUE" = "Positive", "FALSE" = "Negative"),
        name = "SHAP Value"
      ) +
      scale_y_continuous(labels = label_func) +
      coord_flip() +
      labs(
        title = sprintf("SHAP Values - %s Instance", category_name),
        subtitle = sprintf("Predicted class: %s", pred_class),
        x = "Feature",
        y = y_axis_label
      ) +
      theme_minimal() +
      theme(
        plot.title = element_text(size = 14, face = "bold"),
        plot.subtitle = element_text(size = 12),
        axis.text.y = element_text(size = 8)
      )
    
    print(p_shap)
  } else {
    cat(sprintf("SHAP values format not recognized for %s instance\n", category_name))
  }
}

cat("\nPer-instance SHAP calculations complete.\n")
```

